{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-244-23a106c8e886>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.layers as tfkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file):\n",
    "    classes = []\n",
    "    names = []\n",
    "    with open(file, 'r') as f:\n",
    "        header = True\n",
    "        for line in f:\n",
    "            if header:\n",
    "                header = False\n",
    "                continue\n",
    "            n, c = line.split(',')\n",
    "            names.append(n)\n",
    "            classes.append(c)\n",
    "    return np.array(names), np.array(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000 training examples.\n",
      "5149 testing examples.\n"
     ]
    }
   ],
   "source": [
    "names, labels = process_file('train_labels.txt')\n",
    "labels = np.array([int(x) for x in labels])\n",
    "names = np.array(names)\n",
    "eval_names, _ = process_file('sample_submission.txt')\n",
    "print(f\"{len(names)} training examples.\")\n",
    "print(f\"{len(eval_names)} testing examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(name):\n",
    "    img = cv2.imread(f\"data/{name}.png\")\n",
    "    img = img.astype(np.float32) / 255.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PIXELS = {}\n",
    "\n",
    "def read_and_process_image(name, verbose=False):\n",
    "    import math\n",
    "    img = read_image(name)\n",
    "    (h, w) = img.shape[:2]\n",
    "    blurred = cv2.medianBlur(img, 7)\n",
    "    _, thresh = cv2.threshold(blurred, np.min(blurred) + 10, 255, cv2.THRESH_BINARY)\n",
    "    thresh_small = cv2.resize(thresh, (thresh.shape[0], thresh.shape[1]), cv2.INTER_NEAREST)\n",
    "    _, labels, stats, _ = cv2.connectedComponentsWithStats(thresh_small, 8, cv2.CV_32S)\n",
    "    chosen_comps = [idx for idx, stat in enumerate(stats) if stat[4] > 2000 and idx != 0]\n",
    "    thresh_clean = np.zeros_like(thresh)\n",
    "    for comp in chosen_comps:\n",
    "        thresh_clean[labels == comp] = 255\n",
    "    coords = np.column_stack(np.where(thresh_clean > 0)).astype(np.float32)\n",
    "    \n",
    "    NUM_PIXELS[name] = np.sum(thresh_clean != 0)\n",
    "    \n",
    "    img_cropped = None\n",
    "#     try:\n",
    "#         coords = np.column_stack(np.where(thresh_clean > 0)).astype(np.float32)\n",
    "#         mean, eigenvectors = cv2.PCACompute(coords, mean=None)\n",
    "#         angle = math.asin(eigenvectors[0][0]) / math.atan(1.0) * 45\n",
    "#         print(eigenvectors)\n",
    "        # find contours / rectangle\n",
    "    try:\n",
    "        contours, _ = cv2.findContours(thresh_clean, 1, 1)\n",
    "        angle = cv2.minAreaRect(contours[0])[2]\n",
    "        if angle < -45:\n",
    "            angle += 90\n",
    "        if angle > 45:\n",
    "            angle -= 90\n",
    "        M = cv2.getRotationMatrix2D((h/2, w/2), angle, 1)\n",
    "        thresh_rot = cv2.warpAffine(thresh_clean, M, (w, h))\n",
    "        coords = np.column_stack(np.where(thresh_rot > 0)).astype(np.float32)\n",
    "        rect = cv2.boundingRect(coords)\n",
    "        rot = cv2.warpAffine(img, M, (w, h), \n",
    "                             flags=cv2.INTER_CUBIC,\n",
    "                             borderMode=cv2.BORDER_REPLICATE)\n",
    "        cropped = rot[ \n",
    "            rect[0]:(rect[0]+rect[2]),\n",
    "            rect[1]:(rect[1]+rect[3]),\n",
    "        ]\n",
    "        cropped = cv2.resize(cropped, (img.shape[0], img.shape[1]))\n",
    "    except IndexError:\n",
    "        cropped = img\n",
    "        \n",
    "        \n",
    "#         print(angle)\n",
    "#         if angle > 45:\n",
    "#             angle -= 90\n",
    "#         print(angle)\n",
    "#         M = cv2.getRotationMatrix2D(tuple(np.ravel(mean)), angle, scale=1.)\n",
    "#         rotated = cv2.warpAffine(img, M, (w, h),\n",
    "#                                  flags=cv2.INTER_CUBIC, \n",
    "#                                  borderMode=cv2.BORDER_REPLICATE)\n",
    "#     except:\n",
    "#         rotated = img\n",
    "    \n",
    "#     try:\n",
    "#         stat = list(sorted(stats, key=lambda x: x[4], reverse=True))[1]\n",
    "#         cropped = img[stat[1]:(stat[1]+stat[3]), stat[0]:(stat[0]+stat[2])]\n",
    "#         cropped = cv2.resize(cropped, (img.shape[0], img.shape[1]))\n",
    "#     except IndexError:\n",
    "#         cropped = img\n",
    "        \n",
    "    if verbose:\n",
    "        print(name)\n",
    "        plt.subplot(1, 5, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.subplot(1, 5, 2)\n",
    "        plt.imshow(blurred)\n",
    "        plt.subplot(1, 5, 3)\n",
    "        plt.imshow(thresh)\n",
    "        plt.subplot(1, 5, 4)\n",
    "        plt.imshow(thresh_clean)\n",
    "        plt.subplot(1, 5, 5)\n",
    "        plt.imshow(cropped)\n",
    "        plt.show()\n",
    "    \n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06596498\n"
     ]
    }
   ],
   "source": [
    "def choose(label):\n",
    "    idx = np.random.choice(np.arange(len(names)))\n",
    "    if labels[idx] != label:\n",
    "        return choose(label)\n",
    "    img_name = names[idx]\n",
    "    return img_name\n",
    "    \n",
    "    return img\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "i0 = choose(0)\n",
    "plt.title(i0)\n",
    "plt.imshow(read_image(i0))\n",
    "plt.subplot(1, 2, 2)\n",
    "i1 = choose(1)\n",
    "plt.title(i1)\n",
    "plt.imshow(read_image(i1))\n",
    "plt.show()\n",
    "print(np.mean(read_image(i1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros, ones = [], []\n",
    "# for name, label in list(zip(names, labels)):\n",
    "#     read_and_process_image(name)\n",
    "#     if label == 0:\n",
    "#         zeros.append(NUM_PIXELS[name])\n",
    "#     else:\n",
    "#         ones.append(NUM_PIXELS[name])\n",
    "# print(f\"MIN: {min(ones)}\")\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.title('ZEROS')\n",
    "# plt.hist(zeros)\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.title('ONES')\n",
    "# plt.hist(ones)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_images = []\n",
    "d_labels = []\n",
    "for idx, name in enumerate(names[:3000]):\n",
    "    img = read_image(name)\n",
    "    if img.min() != img.max():\n",
    "        d_images.append(img)\n",
    "        d_labels.append(labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    import tensorflow.keras.backend as K\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    return tp / (tp + fp + K.epsilon())\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    import tensorflow.keras.backend as K\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    return tp / (tp + fn + K.epsilon())\n",
    "    \n",
    "def f1(y_true, y_pred):\n",
    "    import tensorflow.keras.backend as K\n",
    "    \n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    import tensorflow.keras.backend as K\n",
    "    tp = K.sum(y_true*y_pred, axis=0)\n",
    "    tn = K.sum((1-y_true)*(1-y_pred), axis=0)\n",
    "    fp = K.sum((1-y_true)*y_pred, axis=0)\n",
    "    fn = K.sum(y_true*(1-y_pred), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "    \n",
    "    print(p)\n",
    "    print(r)\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    ret = 1 - K.mean(f1)\n",
    "    print(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(input_shape):\n",
    "    model = tfk.Sequential()\n",
    "    \n",
    "    model.add(tfkl.Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(tfkl.MaxPooling2D(pool_size=2))\n",
    "    model.add(tfkl.Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
    "    model.add(tfkl.MaxPooling2D(pool_size=2))\n",
    "    model.add(tfkl.Conv2D(64, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
    "    \n",
    "    model.add(tfkl.GlobalAveragePooling2D())\n",
    "    model.add(tfkl.Dropout(0.4))\n",
    "    \n",
    "    model.add(tfkl.Dense(32, activation='relu'))\n",
    "    model.add(tfkl.Dropout(0.4))\n",
    "    \n",
    "    model.add(tfkl.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tfk.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = tfkl.Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=tfk.regularizers.l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = tfkl.BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = tfkl.Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = tfkl.BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = tfkl.Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "def resnet_v2(input_shape, depth):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = tfkl.Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = tfkl.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Activation('relu')(x)\n",
    "    x = tfkl.AveragePooling2D(pool_size=8)(x)\n",
    "    y = tfkl.Flatten()(x)\n",
    "    outputs = tfkl.Dense(1,\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = tfk.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet import ResNet50 as ApplicationModel\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "\n",
    "def vgg_model(input_shape):\n",
    "    # load model without classifier layers\n",
    "    vgg_conv = ApplicationModel(include_top=False, input_shape=input_shape)\n",
    "    for layer in vgg_conv.layers:\n",
    "        layer.trainable = False\n",
    "    model = tfk.Sequential()\n",
    "    model.add(vgg_conv)\n",
    "    model.add(tfkl.GlobalAveragePooling2D())\n",
    "    model.add(tfkl.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = my_model((224, 224, 3))\n",
    "# model = resnet_v2(input_shape=X.shape[1:], depth=47)\n",
    "model = vgg_model((224, 224, 3))\n",
    "\n",
    "X = np.array(d_images, dtype=np.float32)\n",
    "y = np.array(d_labels, dtype=np.float32)\n",
    "#X -= X.mean()\n",
    "#X += 128.\n",
    "#X = tfk.applications.densenet.preprocess_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_12  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                131136    \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 23,718,913\n",
      "Trainable params: 131,201\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tfk.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, images, labels, batch_size=32, shuffle=True, batch_count=1):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.images = images\n",
    "        self.batch_count = batch_count\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return self.batch_count\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        print(f\"__getitem__ {index}\")\n",
    "        'Updates indexes after each epoch'\n",
    "        pos_indexes = np.where(self.labels == 1.)\n",
    "        neg_indexes = np.where(self.labels == 0.)\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(pos_indexes)\n",
    "            np.random.shuffle(neg_indexes)\n",
    "        \n",
    "        print(f\"__shuffle__ {index}\")\n",
    "        \n",
    "        indexes = np.ravel(np.column_stack([\n",
    "            pos_indexes[:self.batch_size//2],\n",
    "            neg_indexes[:self.batch_size//2],\n",
    "        ]))\n",
    "        \n",
    "        X = self.images[indexes]\n",
    "        X = X.reshape(X.shape[:3] + (1,))\n",
    "        y = self.labels[indexes]\n",
    "        \n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "\n",
    "        return X, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weight(y):\n",
    "    pos = np.sum(y == 1)\n",
    "    neg = np.sum(y == 0)\n",
    "    total = y.shape[0]\n",
    "    weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "    weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "    return class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "train_datagen = tfk.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range=0.,\n",
    "    zoom_range=0.05,\n",
    "    rotation_range=180,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='constant',\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    cval=0,\n",
    ")\n",
    "val_datagen = tfk.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range=0.,\n",
    "    zoom_range=0.05,\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='constant',\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    cval=0,\n",
    ")\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for X, y in train_datagen.flow(X_train, y_train, batch_size=128):\n",
    "#    for i in range(len(X)):\n",
    "#        print(np.std(X[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 3/70 [>.............................] - ETA: 11:56 - loss: 0.8157 - f1: 0.2798 - precision: 0.1728 - recall: 0.8333 - accuracy: 0.2500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-5c35f27b2850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m#callbacks = [lr_reducer, early_stopping],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n",
      "\u001b[0;32m~/.local/share/virtualenvs/cv-Lb-faxAm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.local/share/virtualenvs/cv-Lb-faxAm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/cv-Lb-faxAm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/.local/share/virtualenvs/cv-Lb-faxAm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/cv-Lb-faxAm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/cv-Lb-faxAm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m           \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[1;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[0;32m~/.local/share/virtualenvs/cv-Lb-faxAm/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/cv-Lb-faxAm/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/cv-Lb-faxAm/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/cv-Lb-faxAm/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    604\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m           data_format=data_format)\n\u001b[0m\u001b[1;32m    607\u001b[0m   ]\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/cv-Lb-faxAm/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[0;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m         explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1188\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss = 'binary_crossentropy'\n",
    "# loss = f1_loss\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[f1, precision, recall, 'accuracy'])\n",
    "early_stopping = tfk.callbacks.EarlyStopping(\n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "lr_reducer = tfk.callbacks.ReduceLROnPlateau(\n",
    "    factor=np.sqrt(0.1),\n",
    "    cooldown=0,\n",
    "    patience=5,\n",
    "    min_lr=0.5e-6,\n",
    ")\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=100, \n",
    "    class_weight=get_class_weight(y), \n",
    "    #callbacks = [lr_reducer, early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, epochs=1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    dataset = dataset.shuffle(1000).batch(32)\n",
    "    print(f\"{X_train.shape[0]} training examples, {X_test.shape[0]} validation.\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch} started.')\n",
    "        loss_history = []\n",
    "        for (batch, (images, labels)) in enumerate(dataset):\n",
    "            if labels.numpy().sum() == 0:\n",
    "                print('Skipping batch -- no positive examples.')\n",
    "                \n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(images, training=True)\n",
    "                loss_value = f1_loss(labels, logits)\n",
    "                loss = loss_value.numpy().mean()\n",
    "                loss_history.append(loss)\n",
    "                grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "        print(f'Epoch {epoch} finished.')\n",
    "        print(f\"Loss: {np.mean(loss_history)}\")\n",
    "        y_pred = tf.math.round(model(X_test))\n",
    "        print(f\"Validation f1: {f1(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(X, y, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "print(y_pred)\n",
    "print(np.mean(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
