{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file):\n",
    "    classes = []\n",
    "    names = []\n",
    "    with open(file, 'r') as f:\n",
    "        header = True\n",
    "        for line in f:\n",
    "            if header:\n",
    "                header = False\n",
    "                continue\n",
    "            n, c = line.split(',')\n",
    "            names.append(n)\n",
    "            classes.append(c)\n",
    "    return np.array(names), np.array(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000 training examples.\n",
      "5149 testing examples.\n"
     ]
    }
   ],
   "source": [
    "names, labels = process_file('train_labels.txt')\n",
    "labels = np.array([int(x) for x in labels])\n",
    "names = np.array(names)\n",
    "eval_names, _ = process_file('sample_submission.txt')\n",
    "print(f\"{len(names)} training examples.\")\n",
    "print(f\"{len(eval_names)} testing examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(name):\n",
    "    img = cv2.imread(f\"data/{name}.png\")\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PIXELS = {}\n",
    "\n",
    "def read_and_process_image(name, verbose=False):\n",
    "    import math\n",
    "    img = read_image(name)\n",
    "    (h, w) = img.shape[:2]\n",
    "    blurred = cv2.medianBlur(img, 7)\n",
    "    _, thresh = cv2.threshold(blurred, np.min(blurred) + 10, 255, cv2.THRESH_BINARY)\n",
    "    thresh_small = cv2.resize(thresh, (thresh.shape[0], thresh.shape[1]), cv2.INTER_NEAREST)\n",
    "    _, labels, stats, _ = cv2.connectedComponentsWithStats(thresh_small, 8, cv2.CV_32S)\n",
    "    chosen_comps = [idx for idx, stat in enumerate(stats) if stat[4] > 2000 and idx != 0]\n",
    "    thresh_clean = np.zeros_like(thresh)\n",
    "    for comp in chosen_comps:\n",
    "        thresh_clean[labels == comp] = 255\n",
    "    coords = np.column_stack(np.where(thresh_clean > 0)).astype(np.float32)\n",
    "    \n",
    "    NUM_PIXELS[name] = np.sum(thresh_clean != 0)\n",
    "    \n",
    "    img_cropped = None\n",
    "#     try:\n",
    "#         coords = np.column_stack(np.where(thresh_clean > 0)).astype(np.float32)\n",
    "#         mean, eigenvectors = cv2.PCACompute(coords, mean=None)\n",
    "#         angle = math.asin(eigenvectors[0][0]) / math.atan(1.0) * 45\n",
    "#         print(eigenvectors)\n",
    "        # find contours / rectangle\n",
    "    try:\n",
    "        contours, _ = cv2.findContours(thresh_clean, 1, 1)\n",
    "        angle = cv2.minAreaRect(contours[0])[2]\n",
    "        if angle < -45:\n",
    "            angle += 90\n",
    "        if angle > 45:\n",
    "            angle -= 90\n",
    "        M = cv2.getRotationMatrix2D((h/2, w/2), angle, 1)\n",
    "        thresh_rot = cv2.warpAffine(thresh_clean, M, (w, h))\n",
    "        coords = np.column_stack(np.where(thresh_rot > 0)).astype(np.float32)\n",
    "        rect = cv2.boundingRect(coords)\n",
    "        rot = cv2.warpAffine(img, M, (w, h), \n",
    "                             flags=cv2.INTER_CUBIC,\n",
    "                             borderMode=cv2.BORDER_REPLICATE)\n",
    "        cropped = rot[ \n",
    "            rect[0]:(rect[0]+rect[2]),\n",
    "            rect[1]:(rect[1]+rect[3]),\n",
    "        ]\n",
    "        cropped = cv2.resize(cropped, (img.shape[0], img.shape[1]))\n",
    "    except IndexError:\n",
    "        cropped = img\n",
    "        \n",
    "        \n",
    "#         print(angle)\n",
    "#         if angle > 45:\n",
    "#             angle -= 90\n",
    "#         print(angle)\n",
    "#         M = cv2.getRotationMatrix2D(tuple(np.ravel(mean)), angle, scale=1.)\n",
    "#         rotated = cv2.warpAffine(img, M, (w, h),\n",
    "#                                  flags=cv2.INTER_CUBIC, \n",
    "#                                  borderMode=cv2.BORDER_REPLICATE)\n",
    "#     except:\n",
    "#         rotated = img\n",
    "    \n",
    "#     try:\n",
    "#         stat = list(sorted(stats, key=lambda x: x[4], reverse=True))[1]\n",
    "#         cropped = img[stat[1]:(stat[1]+stat[3]), stat[0]:(stat[0]+stat[2])]\n",
    "#         cropped = cv2.resize(cropped, (img.shape[0], img.shape[1]))\n",
    "#     except IndexError:\n",
    "#         cropped = img\n",
    "        \n",
    "    if verbose:\n",
    "        print(name)\n",
    "        plt.subplot(1, 5, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.subplot(1, 5, 2)\n",
    "        plt.imshow(blurred)\n",
    "        plt.subplot(1, 5, 3)\n",
    "        plt.imshow(thresh)\n",
    "        plt.subplot(1, 5, 4)\n",
    "        plt.imshow(thresh_clean)\n",
    "        plt.subplot(1, 5, 5)\n",
    "        plt.imshow(cropped)\n",
    "        plt.show()\n",
    "    \n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.2) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/connectedcomponents.cpp:3926: error: (-215:Assertion failed) L.channels() == 1 && I.channels() == 1 in function 'connectedComponents_sub1'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-97f8288d280c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# img_name = '003252'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_process_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-150-2d64c057a451>\u001b[0m in \u001b[0;36mread_and_process_image\u001b[0;34m(name, verbose)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblurred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblurred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTHRESH_BINARY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mthresh_small\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_NEAREST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnectedComponentsWithStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh_small\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCV_32S\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mchosen_comps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2000\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mthresh_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/connectedcomponents.cpp:3926: error: (-215:Assertion failed) L.channels() == 1 && I.channels() == 1 in function 'connectedComponents_sub1'\n"
     ]
    }
   ],
   "source": [
    "img_name = np.random.choice(names)\n",
    "# img_name = '003252'\n",
    "_ = read_and_process_image(img_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros, ones = [], []\n",
    "# for name, label in list(zip(names, labels)):\n",
    "#     read_and_process_image(name)\n",
    "#     if label == 0:\n",
    "#         zeros.append(NUM_PIXELS[name])\n",
    "#     else:\n",
    "#         ones.append(NUM_PIXELS[name])\n",
    "# print(f\"MIN: {min(ones)}\")\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.title('ZEROS')\n",
    "# plt.hist(zeros)\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.title('ONES')\n",
    "# plt.hist(ones)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 224, 224, 3)\n",
      "(3000,)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for name in names[:3000]:\n",
    "    img = read_image(name)\n",
    "    img = img / 255.\n",
    "    img -= np.mean(img)\n",
    "    X.append(img)\n",
    "X = np.array(X, dtype=np.float32)\n",
    "y = np.array(labels[:3000], dtype=np.float32)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.layers as tfkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    import tensorflow.keras.backend as K\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    return tp / (tp + fp + K.epsilon())\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    import tensorflow.keras.backend as K\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    return tp / (tp + fn + K.epsilon())\n",
    "    \n",
    "def f1(y_true, y_pred):\n",
    "    import tensorflow.keras.backend as K\n",
    "    \n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    import tensorflow.keras.backend as K\n",
    "    tp = K.sum(y_true*y_pred, axis=0)\n",
    "    tn = K.sum((1-y_true)*(1-y_pred), axis=0)\n",
    "    fp = K.sum((1-y_true)*y_pred, axis=0)\n",
    "    fn = K.sum(y_true*(1-y_pred), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "    \n",
    "    print(p)\n",
    "    print(r)\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    ret = 1 - K.mean(f1)\n",
    "    print(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model():\n",
    "    model = tfk.Sequential()\n",
    "    model.add(tfkl.Input(shape=X.shape[1:]))\n",
    "    \n",
    "    model.add(tfkl.Conv2D(16, kernel_size=9, strides=1, activation='relu'))\n",
    "    model.add(tfkl.Dropout(0.3))\n",
    "    model.add(tfkl.MaxPooling2D(5))\n",
    "\n",
    "    model.add(tfkl.Conv2D(32, kernel_size=5, strides=1, activation='relu'))\n",
    "    model.add(tfkl.Dropout(0.3))\n",
    "    model.add(tfkl.MaxPooling2D(3))\n",
    "\n",
    "    model.add(tfkl.Conv2D(64, kernel_size=3, strides=1, activation='relu'))\n",
    "    model.add(tfkl.Dropout(0.3))\n",
    "    model.add(tfkl.MaxPooling2D())\n",
    "\n",
    "    model.add(tfkl.Flatten())\n",
    "    model.add(tfkl.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tfk.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = tfkl.Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=tfk.regularizers.l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = tfkl.BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = tfkl.Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = tfkl.BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = tfkl.Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "def resnet_v2(input_shape, depth):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = tfkl.Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = tfkl.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Activation('relu')(x)\n",
    "    x = tfkl.AveragePooling2D(pool_size=8)(x)\n",
    "    y = tfkl.Flatten()(x)\n",
    "    outputs = tfkl.Dense(1,\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = tfk.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_model():\n",
    "    # load model without classifier layers\n",
    "    vgg_conv = tfk.applications.vgg16.VGG16(include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in vgg_conv.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "    model = tfk.Sequential()\n",
    "    model.add(vgg_conv)\n",
    "    model.add(tfkl.Flatten())\n",
    "    model.add(tfkl.Dense(64, activation='relu'))\n",
    "    model.add(tfkl.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = my_model()\n",
    "# model = resnet_v2(input_shape=X.shape[1:], depth=47)\n",
    "model = vgg_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_45 (Flatten)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 64)                1605696   \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 16,320,449\n",
      "Trainable params: 8,685,185\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tfk.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, images, labels, batch_size=32, shuffle=True, batch_count=1):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.images = images\n",
    "        self.batch_count = batch_count\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return self.batch_count\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        print(f\"__getitem__ {index}\")\n",
    "        'Updates indexes after each epoch'\n",
    "        pos_indexes = np.where(self.labels == 1.)\n",
    "        neg_indexes = np.where(self.labels == 0.)\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(pos_indexes)\n",
    "            np.random.shuffle(neg_indexes)\n",
    "        \n",
    "        print(f\"__shuffle__ {index}\")\n",
    "        \n",
    "        indexes = np.ravel(np.column_stack([\n",
    "            pos_indexes[:self.batch_size//2],\n",
    "            neg_indexes[:self.batch_size//2],\n",
    "        ]))\n",
    "        \n",
    "        X = self.images[indexes]\n",
    "        X = X.reshape(X.shape[:3] + (1,))\n",
    "        y = self.labels[indexes]\n",
    "        \n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "\n",
    "        return X, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weight(y):\n",
    "    pos = np.sum(y == 1)\n",
    "    neg = np.sum(y == 0)\n",
    "    total = y.shape[0]\n",
    "    weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "    weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "    return class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250 samples, validate on 750 samples\n",
      "Epoch 1/100\n",
      " 640/2250 [=======>......................] - ETA: 1:35 - loss: 0.6627 - f1: 0.1475 - precision: 0.1118 - recall: 0.2901 - accuracy: 0.7281"
     ]
    }
   ],
   "source": [
    "model = my_model()\n",
    "loss = 'binary_crossentropy'\n",
    "# loss = f1_loss\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[f1, precision, recall, 'accuracy'])\n",
    "early_stopping = tfk.callbacks.EarlyStopping(\n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "lr_reducer = tfk.callbacks.ReduceLROnPlateau(\n",
    "    factor=np.sqrt(0.1),\n",
    "    cooldown=0,\n",
    "    patience=5,\n",
    "    min_lr=0.5e-6,\n",
    ")\n",
    "model.fit(\n",
    "    X, y, \n",
    "    batch_size=32, \n",
    "    epochs=100, \n",
    "    class_weight=get_class_weight(y), \n",
    "    callbacks = [lr_reducer, early_stopping],\n",
    "    validation_split=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, epochs=1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    dataset = dataset.shuffle(1000).batch(32)\n",
    "    print(f\"{X_train.shape[0]} training examples, {X_test.shape[0]} validation.\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch} started.')\n",
    "        loss_history = []\n",
    "        for (batch, (images, labels)) in enumerate(dataset):\n",
    "            if labels.numpy().sum() == 0:\n",
    "                print('Skipping batch -- no positive examples.')\n",
    "                \n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(images, training=True)\n",
    "                loss_value = f1_loss(labels, logits)\n",
    "                loss = loss_value.numpy().mean()\n",
    "                loss_history.append(loss)\n",
    "                grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "        print(f'Epoch {epoch} finished.')\n",
    "        print(f\"Loss: {np.mean(loss_history)}\")\n",
    "        y_pred = tf.math.round(model(X_test))\n",
    "        print(f\"Validation f1: {f1(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(X, y, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "print(y_pred)\n",
    "print(np.mean(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
